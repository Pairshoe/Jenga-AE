{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3252032520325203,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008130081300813008,
      "grad_norm": 1744.0,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 91.2252,
      "step": 1
    },
    {
      "epoch": 0.0016260162601626016,
      "grad_norm": 1712.0,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 91.1391,
      "step": 2
    },
    {
      "epoch": 0.0024390243902439024,
      "grad_norm": 1720.0,
      "learning_rate": 3e-06,
      "loss": 90.2344,
      "step": 3
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 1720.0,
      "learning_rate": 4.000000000000001e-06,
      "loss": 89.2616,
      "step": 4
    },
    {
      "epoch": 0.0040650406504065045,
      "grad_norm": 1696.0,
      "learning_rate": 5e-06,
      "loss": 86.5816,
      "step": 5
    },
    {
      "epoch": 0.004878048780487805,
      "grad_norm": 1648.0,
      "learning_rate": 6e-06,
      "loss": 84.0806,
      "step": 6
    },
    {
      "epoch": 0.005691056910569106,
      "grad_norm": 1608.0,
      "learning_rate": 7e-06,
      "loss": 79.0877,
      "step": 7
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 1472.0,
      "learning_rate": 8.000000000000001e-06,
      "loss": 71.1616,
      "step": 8
    },
    {
      "epoch": 0.007317073170731708,
      "grad_norm": 1408.0,
      "learning_rate": 9e-06,
      "loss": 67.6562,
      "step": 9
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 1304.0,
      "learning_rate": 1e-05,
      "loss": 57.8602,
      "step": 10
    },
    {
      "epoch": 0.00894308943089431,
      "grad_norm": 1264.0,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 48.732,
      "step": 11
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 1280.0,
      "learning_rate": 1.2e-05,
      "loss": 42.1721,
      "step": 12
    },
    {
      "epoch": 0.01056910569105691,
      "grad_norm": 1160.0,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 35.1304,
      "step": 13
    },
    {
      "epoch": 0.011382113821138212,
      "grad_norm": 1056.0,
      "learning_rate": 1.4e-05,
      "loss": 30.7981,
      "step": 14
    },
    {
      "epoch": 0.012195121951219513,
      "grad_norm": 1128.0,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 31.5392,
      "step": 15
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 1160.0,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 32.5808,
      "step": 16
    },
    {
      "epoch": 0.013821138211382113,
      "grad_norm": 1320.0,
      "learning_rate": 1.7e-05,
      "loss": 37.6691,
      "step": 17
    },
    {
      "epoch": 0.014634146341463415,
      "grad_norm": 1224.0,
      "learning_rate": 1.8e-05,
      "loss": 38.2166,
      "step": 18
    },
    {
      "epoch": 0.015447154471544716,
      "grad_norm": 1304.0,
      "learning_rate": 1.9e-05,
      "loss": 41.9774,
      "step": 19
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 1280.0,
      "learning_rate": 2e-05,
      "loss": 41.0728,
      "step": 20
    },
    {
      "epoch": 0.01707317073170732,
      "grad_norm": 1160.0,
      "learning_rate": 2e-05,
      "loss": 38.343,
      "step": 21
    },
    {
      "epoch": 0.01788617886178862,
      "grad_norm": 1216.0,
      "learning_rate": 2e-05,
      "loss": 37.9719,
      "step": 22
    },
    {
      "epoch": 0.01869918699186992,
      "grad_norm": 1216.0,
      "learning_rate": 2e-05,
      "loss": 35.8233,
      "step": 23
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 1016.0,
      "learning_rate": 2e-05,
      "loss": 31.2403,
      "step": 24
    },
    {
      "epoch": 0.02032520325203252,
      "grad_norm": 944.0,
      "learning_rate": 2e-05,
      "loss": 28.6938,
      "step": 25
    },
    {
      "epoch": 0.02113821138211382,
      "grad_norm": 872.0,
      "learning_rate": 2e-05,
      "loss": 25.7339,
      "step": 26
    },
    {
      "epoch": 0.02195121951219512,
      "grad_norm": 948.0,
      "learning_rate": 2e-05,
      "loss": 27.5404,
      "step": 27
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 1000.0,
      "learning_rate": 2e-05,
      "loss": 28.0606,
      "step": 28
    },
    {
      "epoch": 0.023577235772357725,
      "grad_norm": 948.0,
      "learning_rate": 2e-05,
      "loss": 27.5268,
      "step": 29
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 936.0,
      "learning_rate": 2e-05,
      "loss": 27.9589,
      "step": 30
    },
    {
      "epoch": 0.025203252032520326,
      "grad_norm": 896.0,
      "learning_rate": 2e-05,
      "loss": 27.8449,
      "step": 31
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 892.0,
      "learning_rate": 2e-05,
      "loss": 26.2863,
      "step": 32
    },
    {
      "epoch": 0.026829268292682926,
      "grad_norm": 740.0,
      "learning_rate": 2e-05,
      "loss": 25.1402,
      "step": 33
    },
    {
      "epoch": 0.027642276422764227,
      "grad_norm": 684.0,
      "learning_rate": 2e-05,
      "loss": 24.3721,
      "step": 34
    },
    {
      "epoch": 0.028455284552845527,
      "grad_norm": 556.0,
      "learning_rate": 2e-05,
      "loss": 23.0265,
      "step": 35
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 498.0,
      "learning_rate": 2e-05,
      "loss": 21.698,
      "step": 36
    },
    {
      "epoch": 0.03008130081300813,
      "grad_norm": 544.0,
      "learning_rate": 2e-05,
      "loss": 22.4525,
      "step": 37
    },
    {
      "epoch": 0.030894308943089432,
      "grad_norm": 512.0,
      "learning_rate": 2e-05,
      "loss": 21.9694,
      "step": 38
    },
    {
      "epoch": 0.03170731707317073,
      "grad_norm": 592.0,
      "learning_rate": 2e-05,
      "loss": 22.9199,
      "step": 39
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 568.0,
      "learning_rate": 2e-05,
      "loss": 22.5661,
      "step": 40
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 568.0,
      "learning_rate": 2e-05,
      "loss": 21.9686,
      "step": 41
    },
    {
      "epoch": 0.03414634146341464,
      "grad_norm": 478.0,
      "learning_rate": 2e-05,
      "loss": 21.8251,
      "step": 42
    },
    {
      "epoch": 0.034959349593495934,
      "grad_norm": 434.0,
      "learning_rate": 2e-05,
      "loss": 20.6035,
      "step": 43
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 414.0,
      "learning_rate": 2e-05,
      "loss": 20.8745,
      "step": 44
    },
    {
      "epoch": 0.036585365853658534,
      "grad_norm": 452.0,
      "learning_rate": 2e-05,
      "loss": 21.2007,
      "step": 45
    },
    {
      "epoch": 0.03739837398373984,
      "grad_norm": 448.0,
      "learning_rate": 2e-05,
      "loss": 21.1567,
      "step": 46
    },
    {
      "epoch": 0.038211382113821135,
      "grad_norm": 478.0,
      "learning_rate": 2e-05,
      "loss": 20.65,
      "step": 47
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 512.0,
      "learning_rate": 2e-05,
      "loss": 20.8143,
      "step": 48
    },
    {
      "epoch": 0.03983739837398374,
      "grad_norm": 384.0,
      "learning_rate": 2e-05,
      "loss": 20.3961,
      "step": 49
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 294.0,
      "learning_rate": 2e-05,
      "loss": 19.1834,
      "step": 50
    },
    {
      "epoch": 0.041463414634146344,
      "grad_norm": 378.0,
      "learning_rate": 2e-05,
      "loss": 19.9299,
      "step": 51
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 358.0,
      "learning_rate": 2e-05,
      "loss": 20.1732,
      "step": 52
    },
    {
      "epoch": 0.043089430894308944,
      "grad_norm": 506.0,
      "learning_rate": 2e-05,
      "loss": 21.1279,
      "step": 53
    },
    {
      "epoch": 0.04390243902439024,
      "grad_norm": 402.0,
      "learning_rate": 2e-05,
      "loss": 19.6274,
      "step": 54
    },
    {
      "epoch": 0.044715447154471545,
      "grad_norm": 358.0,
      "learning_rate": 2e-05,
      "loss": 20.3443,
      "step": 55
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 251.0,
      "learning_rate": 2e-05,
      "loss": 19.6289,
      "step": 56
    },
    {
      "epoch": 0.046341463414634146,
      "grad_norm": 356.0,
      "learning_rate": 2e-05,
      "loss": 19.7603,
      "step": 57
    },
    {
      "epoch": 0.04715447154471545,
      "grad_norm": 454.0,
      "learning_rate": 2e-05,
      "loss": 19.5399,
      "step": 58
    },
    {
      "epoch": 0.04796747967479675,
      "grad_norm": 396.0,
      "learning_rate": 2e-05,
      "loss": 20.024,
      "step": 59
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 416.0,
      "learning_rate": 2e-05,
      "loss": 18.7201,
      "step": 60
    },
    {
      "epoch": 0.04959349593495935,
      "grad_norm": 368.0,
      "learning_rate": 2e-05,
      "loss": 19.2905,
      "step": 61
    },
    {
      "epoch": 0.05040650406504065,
      "grad_norm": 306.0,
      "learning_rate": 2e-05,
      "loss": 19.069,
      "step": 62
    },
    {
      "epoch": 0.05121951219512195,
      "grad_norm": 260.0,
      "learning_rate": 2e-05,
      "loss": 18.8115,
      "step": 63
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 358.0,
      "learning_rate": 2e-05,
      "loss": 18.5435,
      "step": 64
    },
    {
      "epoch": 0.052845528455284556,
      "grad_norm": 326.0,
      "learning_rate": 2e-05,
      "loss": 18.9594,
      "step": 65
    },
    {
      "epoch": 0.05365853658536585,
      "grad_norm": 338.0,
      "learning_rate": 2e-05,
      "loss": 19.105,
      "step": 66
    },
    {
      "epoch": 0.05447154471544716,
      "grad_norm": 312.0,
      "learning_rate": 2e-05,
      "loss": 18.9722,
      "step": 67
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 288.0,
      "learning_rate": 2e-05,
      "loss": 18.3212,
      "step": 68
    },
    {
      "epoch": 0.05609756097560976,
      "grad_norm": 314.0,
      "learning_rate": 2e-05,
      "loss": 18.5253,
      "step": 69
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 304.0,
      "learning_rate": 2e-05,
      "loss": 18.8316,
      "step": 70
    },
    {
      "epoch": 0.05772357723577236,
      "grad_norm": 330.0,
      "learning_rate": 2e-05,
      "loss": 18.597,
      "step": 71
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 320.0,
      "learning_rate": 2e-05,
      "loss": 18.6184,
      "step": 72
    },
    {
      "epoch": 0.05934959349593496,
      "grad_norm": 442.0,
      "learning_rate": 2e-05,
      "loss": 18.6652,
      "step": 73
    },
    {
      "epoch": 0.06016260162601626,
      "grad_norm": 306.0,
      "learning_rate": 2e-05,
      "loss": 17.8687,
      "step": 74
    },
    {
      "epoch": 0.06097560975609756,
      "grad_norm": 234.0,
      "learning_rate": 2e-05,
      "loss": 17.9706,
      "step": 75
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 292.0,
      "learning_rate": 2e-05,
      "loss": 17.5436,
      "step": 76
    },
    {
      "epoch": 0.06260162601626017,
      "grad_norm": 304.0,
      "learning_rate": 2e-05,
      "loss": 18.4262,
      "step": 77
    },
    {
      "epoch": 0.06341463414634146,
      "grad_norm": 284.0,
      "learning_rate": 2e-05,
      "loss": 16.9564,
      "step": 78
    },
    {
      "epoch": 0.06422764227642276,
      "grad_norm": 276.0,
      "learning_rate": 2e-05,
      "loss": 17.5082,
      "step": 79
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 344.0,
      "learning_rate": 2e-05,
      "loss": 17.9781,
      "step": 80
    },
    {
      "epoch": 0.06585365853658537,
      "grad_norm": 250.0,
      "learning_rate": 2e-05,
      "loss": 17.893,
      "step": 81
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 316.0,
      "learning_rate": 2e-05,
      "loss": 17.7466,
      "step": 82
    },
    {
      "epoch": 0.06747967479674796,
      "grad_norm": 312.0,
      "learning_rate": 2e-05,
      "loss": 17.6912,
      "step": 83
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 368.0,
      "learning_rate": 2e-05,
      "loss": 17.7467,
      "step": 84
    },
    {
      "epoch": 0.06910569105691057,
      "grad_norm": 352.0,
      "learning_rate": 2e-05,
      "loss": 17.1557,
      "step": 85
    },
    {
      "epoch": 0.06991869918699187,
      "grad_norm": 358.0,
      "learning_rate": 2e-05,
      "loss": 18.0767,
      "step": 86
    },
    {
      "epoch": 0.07073170731707316,
      "grad_norm": 320.0,
      "learning_rate": 2e-05,
      "loss": 17.5935,
      "step": 87
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 262.0,
      "learning_rate": 2e-05,
      "loss": 16.8624,
      "step": 88
    },
    {
      "epoch": 0.07235772357723577,
      "grad_norm": 318.0,
      "learning_rate": 2e-05,
      "loss": 17.5174,
      "step": 89
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 246.0,
      "learning_rate": 2e-05,
      "loss": 17.276,
      "step": 90
    },
    {
      "epoch": 0.07398373983739838,
      "grad_norm": 296.0,
      "learning_rate": 2e-05,
      "loss": 17.064,
      "step": 91
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 278.0,
      "learning_rate": 2e-05,
      "loss": 17.3174,
      "step": 92
    },
    {
      "epoch": 0.07560975609756097,
      "grad_norm": 316.0,
      "learning_rate": 2e-05,
      "loss": 16.8132,
      "step": 93
    },
    {
      "epoch": 0.07642276422764227,
      "grad_norm": 233.0,
      "learning_rate": 2e-05,
      "loss": 16.8475,
      "step": 94
    },
    {
      "epoch": 0.07723577235772358,
      "grad_norm": 218.0,
      "learning_rate": 2e-05,
      "loss": 16.6636,
      "step": 95
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 282.0,
      "learning_rate": 2e-05,
      "loss": 16.6107,
      "step": 96
    },
    {
      "epoch": 0.07886178861788617,
      "grad_norm": 302.0,
      "learning_rate": 2e-05,
      "loss": 16.9356,
      "step": 97
    },
    {
      "epoch": 0.07967479674796749,
      "grad_norm": 217.0,
      "learning_rate": 2e-05,
      "loss": 16.5907,
      "step": 98
    },
    {
      "epoch": 0.08048780487804878,
      "grad_norm": 338.0,
      "learning_rate": 2e-05,
      "loss": 16.6861,
      "step": 99
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 260.0,
      "learning_rate": 2e-05,
      "loss": 16.487,
      "step": 100
    },
    {
      "epoch": 0.08211382113821138,
      "grad_norm": 288.0,
      "learning_rate": 2e-05,
      "loss": 16.4844,
      "step": 101
    },
    {
      "epoch": 0.08292682926829269,
      "grad_norm": 400.0,
      "learning_rate": 2e-05,
      "loss": 16.7921,
      "step": 102
    },
    {
      "epoch": 0.08373983739837398,
      "grad_norm": 388.0,
      "learning_rate": 2e-05,
      "loss": 16.398,
      "step": 103
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 426.0,
      "learning_rate": 2e-05,
      "loss": 17.1109,
      "step": 104
    },
    {
      "epoch": 0.08536585365853659,
      "grad_norm": 438.0,
      "learning_rate": 2e-05,
      "loss": 16.8206,
      "step": 105
    },
    {
      "epoch": 0.08617886178861789,
      "grad_norm": 378.0,
      "learning_rate": 2e-05,
      "loss": 15.9701,
      "step": 106
    },
    {
      "epoch": 0.08699186991869919,
      "grad_norm": 482.0,
      "learning_rate": 2e-05,
      "loss": 16.6286,
      "step": 107
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 336.0,
      "learning_rate": 2e-05,
      "loss": 16.6521,
      "step": 108
    },
    {
      "epoch": 0.0886178861788618,
      "grad_norm": 318.0,
      "learning_rate": 2e-05,
      "loss": 16.5467,
      "step": 109
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 326.0,
      "learning_rate": 2e-05,
      "loss": 16.0043,
      "step": 110
    },
    {
      "epoch": 0.09024390243902439,
      "grad_norm": 334.0,
      "learning_rate": 2e-05,
      "loss": 16.1323,
      "step": 111
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 167.0,
      "learning_rate": 2e-05,
      "loss": 15.3618,
      "step": 112
    },
    {
      "epoch": 0.091869918699187,
      "grad_norm": 247.0,
      "learning_rate": 2e-05,
      "loss": 16.3491,
      "step": 113
    },
    {
      "epoch": 0.09268292682926829,
      "grad_norm": 302.0,
      "learning_rate": 2e-05,
      "loss": 14.755,
      "step": 114
    },
    {
      "epoch": 0.09349593495934959,
      "grad_norm": 304.0,
      "learning_rate": 2e-05,
      "loss": 15.836,
      "step": 115
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 200.0,
      "learning_rate": 2e-05,
      "loss": 15.8823,
      "step": 116
    },
    {
      "epoch": 0.0951219512195122,
      "grad_norm": 188.0,
      "learning_rate": 2e-05,
      "loss": 16.1698,
      "step": 117
    },
    {
      "epoch": 0.0959349593495935,
      "grad_norm": 202.0,
      "learning_rate": 2e-05,
      "loss": 15.7517,
      "step": 118
    },
    {
      "epoch": 0.09674796747967479,
      "grad_norm": 163.0,
      "learning_rate": 2e-05,
      "loss": 15.7148,
      "step": 119
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 14.7035,
      "step": 120
    },
    {
      "epoch": 0.0983739837398374,
      "grad_norm": 153.0,
      "learning_rate": 2e-05,
      "loss": 15.5156,
      "step": 121
    },
    {
      "epoch": 0.0991869918699187,
      "grad_norm": 180.0,
      "learning_rate": 2e-05,
      "loss": 15.5154,
      "step": 122
    },
    {
      "epoch": 0.1,
      "grad_norm": 159.0,
      "learning_rate": 2e-05,
      "loss": 15.3238,
      "step": 123
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 141.0,
      "learning_rate": 2e-05,
      "loss": 15.6323,
      "step": 124
    },
    {
      "epoch": 0.1016260162601626,
      "grad_norm": 161.0,
      "learning_rate": 2e-05,
      "loss": 15.2732,
      "step": 125
    },
    {
      "epoch": 0.1024390243902439,
      "grad_norm": 210.0,
      "learning_rate": 2e-05,
      "loss": 15.2237,
      "step": 126
    },
    {
      "epoch": 0.10325203252032521,
      "grad_norm": 213.0,
      "learning_rate": 2e-05,
      "loss": 15.8021,
      "step": 127
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 139.0,
      "learning_rate": 2e-05,
      "loss": 15.5066,
      "step": 128
    },
    {
      "epoch": 0.1048780487804878,
      "grad_norm": 278.0,
      "learning_rate": 2e-05,
      "loss": 14.716,
      "step": 129
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 176.0,
      "learning_rate": 2e-05,
      "loss": 15.2636,
      "step": 130
    },
    {
      "epoch": 0.10650406504065041,
      "grad_norm": 153.0,
      "learning_rate": 2e-05,
      "loss": 15.436,
      "step": 131
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 144.0,
      "learning_rate": 2e-05,
      "loss": 15.0607,
      "step": 132
    },
    {
      "epoch": 0.108130081300813,
      "grad_norm": 207.0,
      "learning_rate": 2e-05,
      "loss": 15.2028,
      "step": 133
    },
    {
      "epoch": 0.10894308943089431,
      "grad_norm": 185.0,
      "learning_rate": 2e-05,
      "loss": 15.8639,
      "step": 134
    },
    {
      "epoch": 0.10975609756097561,
      "grad_norm": 122.5,
      "learning_rate": 2e-05,
      "loss": 15.6144,
      "step": 135
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 236.0,
      "learning_rate": 2e-05,
      "loss": 15.1536,
      "step": 136
    },
    {
      "epoch": 0.11138211382113822,
      "grad_norm": 178.0,
      "learning_rate": 2e-05,
      "loss": 15.2552,
      "step": 137
    },
    {
      "epoch": 0.11219512195121951,
      "grad_norm": 213.0,
      "learning_rate": 2e-05,
      "loss": 15.6238,
      "step": 138
    },
    {
      "epoch": 0.11300813008130081,
      "grad_norm": 181.0,
      "learning_rate": 2e-05,
      "loss": 14.4998,
      "step": 139
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 134.0,
      "learning_rate": 2e-05,
      "loss": 15.2897,
      "step": 140
    },
    {
      "epoch": 0.11463414634146342,
      "grad_norm": 142.0,
      "learning_rate": 2e-05,
      "loss": 15.5938,
      "step": 141
    },
    {
      "epoch": 0.11544715447154472,
      "grad_norm": 169.0,
      "learning_rate": 2e-05,
      "loss": 15.0183,
      "step": 142
    },
    {
      "epoch": 0.11626016260162601,
      "grad_norm": 129.0,
      "learning_rate": 2e-05,
      "loss": 15.236,
      "step": 143
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 117.0,
      "learning_rate": 2e-05,
      "loss": 14.8689,
      "step": 144
    },
    {
      "epoch": 0.11788617886178862,
      "grad_norm": 172.0,
      "learning_rate": 2e-05,
      "loss": 15.4629,
      "step": 145
    },
    {
      "epoch": 0.11869918699186992,
      "grad_norm": 162.0,
      "learning_rate": 2e-05,
      "loss": 15.0061,
      "step": 146
    },
    {
      "epoch": 0.11951219512195121,
      "grad_norm": 155.0,
      "learning_rate": 2e-05,
      "loss": 14.5956,
      "step": 147
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 138.0,
      "learning_rate": 2e-05,
      "loss": 15.5327,
      "step": 148
    },
    {
      "epoch": 0.12113821138211382,
      "grad_norm": 136.0,
      "learning_rate": 2e-05,
      "loss": 15.332,
      "step": 149
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 115.0,
      "learning_rate": 2e-05,
      "loss": 15.2444,
      "step": 150
    },
    {
      "epoch": 0.12276422764227642,
      "grad_norm": 171.0,
      "learning_rate": 2e-05,
      "loss": 14.2125,
      "step": 151
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 187.0,
      "learning_rate": 2e-05,
      "loss": 15.1945,
      "step": 152
    },
    {
      "epoch": 0.12439024390243902,
      "grad_norm": 118.5,
      "learning_rate": 2e-05,
      "loss": 15.5054,
      "step": 153
    },
    {
      "epoch": 0.12520325203252033,
      "grad_norm": 159.0,
      "learning_rate": 2e-05,
      "loss": 15.0183,
      "step": 154
    },
    {
      "epoch": 0.12601626016260162,
      "grad_norm": 213.0,
      "learning_rate": 2e-05,
      "loss": 15.108,
      "step": 155
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 124.5,
      "learning_rate": 2e-05,
      "loss": 15.206,
      "step": 156
    },
    {
      "epoch": 0.12764227642276424,
      "grad_norm": 139.0,
      "learning_rate": 2e-05,
      "loss": 15.6439,
      "step": 157
    },
    {
      "epoch": 0.12845528455284552,
      "grad_norm": 209.0,
      "learning_rate": 2e-05,
      "loss": 15.0581,
      "step": 158
    },
    {
      "epoch": 0.12926829268292683,
      "grad_norm": 151.0,
      "learning_rate": 2e-05,
      "loss": 14.9686,
      "step": 159
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 95.0,
      "learning_rate": 2e-05,
      "loss": 15.1381,
      "step": 160
    },
    {
      "epoch": 0.13089430894308943,
      "grad_norm": 136.0,
      "learning_rate": 2e-05,
      "loss": 15.0835,
      "step": 161
    },
    {
      "epoch": 0.13170731707317074,
      "grad_norm": 169.0,
      "learning_rate": 2e-05,
      "loss": 15.3588,
      "step": 162
    },
    {
      "epoch": 0.13252032520325202,
      "grad_norm": 124.5,
      "learning_rate": 2e-05,
      "loss": 15.0957,
      "step": 163
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 122.0,
      "learning_rate": 2e-05,
      "loss": 15.545,
      "step": 164
    },
    {
      "epoch": 0.13414634146341464,
      "grad_norm": 156.0,
      "learning_rate": 2e-05,
      "loss": 14.9044,
      "step": 165
    },
    {
      "epoch": 0.13495934959349593,
      "grad_norm": 130.0,
      "learning_rate": 2e-05,
      "loss": 14.8025,
      "step": 166
    },
    {
      "epoch": 0.13577235772357724,
      "grad_norm": 145.0,
      "learning_rate": 2e-05,
      "loss": 14.8618,
      "step": 167
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 95.5,
      "learning_rate": 2e-05,
      "loss": 15.3149,
      "step": 168
    },
    {
      "epoch": 0.13739837398373983,
      "grad_norm": 114.0,
      "learning_rate": 2e-05,
      "loss": 14.6816,
      "step": 169
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 114.0,
      "learning_rate": 2e-05,
      "loss": 15.2749,
      "step": 170
    },
    {
      "epoch": 0.13902439024390245,
      "grad_norm": 139.0,
      "learning_rate": 2e-05,
      "loss": 15.0169,
      "step": 171
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 107.5,
      "learning_rate": 2e-05,
      "loss": 14.8719,
      "step": 172
    },
    {
      "epoch": 0.14065040650406505,
      "grad_norm": 118.5,
      "learning_rate": 2e-05,
      "loss": 14.4387,
      "step": 173
    },
    {
      "epoch": 0.14146341463414633,
      "grad_norm": 116.0,
      "learning_rate": 2e-05,
      "loss": 14.8001,
      "step": 174
    },
    {
      "epoch": 0.14227642276422764,
      "grad_norm": 127.0,
      "learning_rate": 2e-05,
      "loss": 14.8525,
      "step": 175
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 199.0,
      "learning_rate": 2e-05,
      "loss": 14.5607,
      "step": 176
    },
    {
      "epoch": 0.14390243902439023,
      "grad_norm": 125.5,
      "learning_rate": 2e-05,
      "loss": 14.867,
      "step": 177
    },
    {
      "epoch": 0.14471544715447154,
      "grad_norm": 108.0,
      "learning_rate": 2e-05,
      "loss": 14.6518,
      "step": 178
    },
    {
      "epoch": 0.14552845528455285,
      "grad_norm": 161.0,
      "learning_rate": 2e-05,
      "loss": 14.9788,
      "step": 179
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 143.0,
      "learning_rate": 2e-05,
      "loss": 15.1922,
      "step": 180
    },
    {
      "epoch": 0.14715447154471545,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 15.1487,
      "step": 181
    },
    {
      "epoch": 0.14796747967479676,
      "grad_norm": 153.0,
      "learning_rate": 2e-05,
      "loss": 14.641,
      "step": 182
    },
    {
      "epoch": 0.14878048780487804,
      "grad_norm": 141.0,
      "learning_rate": 2e-05,
      "loss": 14.9046,
      "step": 183
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 160.0,
      "learning_rate": 2e-05,
      "loss": 14.6275,
      "step": 184
    },
    {
      "epoch": 0.15040650406504066,
      "grad_norm": 103.5,
      "learning_rate": 2e-05,
      "loss": 14.5392,
      "step": 185
    },
    {
      "epoch": 0.15121951219512195,
      "grad_norm": 149.0,
      "learning_rate": 2e-05,
      "loss": 14.5857,
      "step": 186
    },
    {
      "epoch": 0.15203252032520326,
      "grad_norm": 100.0,
      "learning_rate": 2e-05,
      "loss": 14.3029,
      "step": 187
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 127.0,
      "learning_rate": 2e-05,
      "loss": 14.4477,
      "step": 188
    },
    {
      "epoch": 0.15365853658536585,
      "grad_norm": 122.5,
      "learning_rate": 2e-05,
      "loss": 14.4301,
      "step": 189
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 132.0,
      "learning_rate": 2e-05,
      "loss": 14.6215,
      "step": 190
    },
    {
      "epoch": 0.15528455284552845,
      "grad_norm": 151.0,
      "learning_rate": 2e-05,
      "loss": 14.7698,
      "step": 191
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 124.5,
      "learning_rate": 2e-05,
      "loss": 14.3068,
      "step": 192
    },
    {
      "epoch": 0.15691056910569107,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 14.4501,
      "step": 193
    },
    {
      "epoch": 0.15772357723577235,
      "grad_norm": 203.0,
      "learning_rate": 2e-05,
      "loss": 14.8606,
      "step": 194
    },
    {
      "epoch": 0.15853658536585366,
      "grad_norm": 130.0,
      "learning_rate": 2e-05,
      "loss": 14.8477,
      "step": 195
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 14.707,
      "step": 196
    },
    {
      "epoch": 0.16016260162601625,
      "grad_norm": 94.5,
      "learning_rate": 2e-05,
      "loss": 14.5663,
      "step": 197
    },
    {
      "epoch": 0.16097560975609757,
      "grad_norm": 183.0,
      "learning_rate": 2e-05,
      "loss": 14.1342,
      "step": 198
    },
    {
      "epoch": 0.16178861788617885,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 14.6015,
      "step": 199
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 103.0,
      "learning_rate": 2e-05,
      "loss": 14.5644,
      "step": 200
    },
    {
      "epoch": 0.16341463414634147,
      "grad_norm": 127.0,
      "learning_rate": 2e-05,
      "loss": 14.6094,
      "step": 201
    },
    {
      "epoch": 0.16422764227642275,
      "grad_norm": 255.0,
      "learning_rate": 2e-05,
      "loss": 14.7762,
      "step": 202
    },
    {
      "epoch": 0.16504065040650406,
      "grad_norm": 256.0,
      "learning_rate": 2e-05,
      "loss": 14.5356,
      "step": 203
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 221.0,
      "learning_rate": 2e-05,
      "loss": 14.6411,
      "step": 204
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 219.0,
      "learning_rate": 2e-05,
      "loss": 15.0404,
      "step": 205
    },
    {
      "epoch": 0.16747967479674797,
      "grad_norm": 217.0,
      "learning_rate": 2e-05,
      "loss": 14.332,
      "step": 206
    },
    {
      "epoch": 0.16829268292682928,
      "grad_norm": 169.0,
      "learning_rate": 2e-05,
      "loss": 14.1537,
      "step": 207
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 188.0,
      "learning_rate": 2e-05,
      "loss": 14.1796,
      "step": 208
    },
    {
      "epoch": 0.16991869918699187,
      "grad_norm": 186.0,
      "learning_rate": 2e-05,
      "loss": 14.6718,
      "step": 209
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 203.0,
      "learning_rate": 2e-05,
      "loss": 14.5396,
      "step": 210
    },
    {
      "epoch": 0.17154471544715447,
      "grad_norm": 122.0,
      "learning_rate": 2e-05,
      "loss": 14.0239,
      "step": 211
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 188.0,
      "learning_rate": 2e-05,
      "loss": 13.9714,
      "step": 212
    },
    {
      "epoch": 0.17317073170731706,
      "grad_norm": 142.0,
      "learning_rate": 2e-05,
      "loss": 14.247,
      "step": 213
    },
    {
      "epoch": 0.17398373983739837,
      "grad_norm": 183.0,
      "learning_rate": 2e-05,
      "loss": 14.4993,
      "step": 214
    },
    {
      "epoch": 0.17479674796747968,
      "grad_norm": 129.0,
      "learning_rate": 2e-05,
      "loss": 14.4412,
      "step": 215
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 134.0,
      "learning_rate": 2e-05,
      "loss": 14.6813,
      "step": 216
    },
    {
      "epoch": 0.17642276422764228,
      "grad_norm": 113.0,
      "learning_rate": 2e-05,
      "loss": 14.6191,
      "step": 217
    },
    {
      "epoch": 0.1772357723577236,
      "grad_norm": 181.0,
      "learning_rate": 2e-05,
      "loss": 14.3081,
      "step": 218
    },
    {
      "epoch": 0.17804878048780487,
      "grad_norm": 159.0,
      "learning_rate": 2e-05,
      "loss": 14.3297,
      "step": 219
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 207.0,
      "learning_rate": 2e-05,
      "loss": 14.3184,
      "step": 220
    },
    {
      "epoch": 0.1796747967479675,
      "grad_norm": 156.0,
      "learning_rate": 2e-05,
      "loss": 14.2142,
      "step": 221
    },
    {
      "epoch": 0.18048780487804877,
      "grad_norm": 146.0,
      "learning_rate": 2e-05,
      "loss": 14.2731,
      "step": 222
    },
    {
      "epoch": 0.18130081300813009,
      "grad_norm": 140.0,
      "learning_rate": 2e-05,
      "loss": 14.0892,
      "step": 223
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 152.0,
      "learning_rate": 2e-05,
      "loss": 14.2375,
      "step": 224
    },
    {
      "epoch": 0.18292682926829268,
      "grad_norm": 264.0,
      "learning_rate": 2e-05,
      "loss": 13.623,
      "step": 225
    },
    {
      "epoch": 0.183739837398374,
      "grad_norm": 130.0,
      "learning_rate": 2e-05,
      "loss": 14.2909,
      "step": 226
    },
    {
      "epoch": 0.18455284552845527,
      "grad_norm": 116.0,
      "learning_rate": 2e-05,
      "loss": 14.3174,
      "step": 227
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 141.0,
      "learning_rate": 2e-05,
      "loss": 14.0845,
      "step": 228
    },
    {
      "epoch": 0.1861788617886179,
      "grad_norm": 109.0,
      "learning_rate": 2e-05,
      "loss": 14.094,
      "step": 229
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 14.6184,
      "step": 230
    },
    {
      "epoch": 0.1878048780487805,
      "grad_norm": 118.5,
      "learning_rate": 2e-05,
      "loss": 14.4219,
      "step": 231
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 99.0,
      "learning_rate": 2e-05,
      "loss": 14.2825,
      "step": 232
    },
    {
      "epoch": 0.18943089430894308,
      "grad_norm": 102.5,
      "learning_rate": 2e-05,
      "loss": 14.4126,
      "step": 233
    },
    {
      "epoch": 0.1902439024390244,
      "grad_norm": 93.5,
      "learning_rate": 2e-05,
      "loss": 14.2331,
      "step": 234
    },
    {
      "epoch": 0.1910569105691057,
      "grad_norm": 104.0,
      "learning_rate": 2e-05,
      "loss": 13.9884,
      "step": 235
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 151.0,
      "learning_rate": 2e-05,
      "loss": 14.0912,
      "step": 236
    },
    {
      "epoch": 0.1926829268292683,
      "grad_norm": 142.0,
      "learning_rate": 2e-05,
      "loss": 14.2956,
      "step": 237
    },
    {
      "epoch": 0.19349593495934958,
      "grad_norm": 134.0,
      "learning_rate": 2e-05,
      "loss": 14.1044,
      "step": 238
    },
    {
      "epoch": 0.1943089430894309,
      "grad_norm": 122.5,
      "learning_rate": 2e-05,
      "loss": 14.5441,
      "step": 239
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 99.0,
      "learning_rate": 2e-05,
      "loss": 14.4582,
      "step": 240
    },
    {
      "epoch": 0.19593495934959348,
      "grad_norm": 119.0,
      "learning_rate": 2e-05,
      "loss": 14.6174,
      "step": 241
    },
    {
      "epoch": 0.1967479674796748,
      "grad_norm": 143.0,
      "learning_rate": 2e-05,
      "loss": 14.2062,
      "step": 242
    },
    {
      "epoch": 0.1975609756097561,
      "grad_norm": 164.0,
      "learning_rate": 2e-05,
      "loss": 14.0948,
      "step": 243
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 210.0,
      "learning_rate": 2e-05,
      "loss": 14.7274,
      "step": 244
    },
    {
      "epoch": 0.1991869918699187,
      "grad_norm": 97.5,
      "learning_rate": 2e-05,
      "loss": 14.3231,
      "step": 245
    },
    {
      "epoch": 0.2,
      "grad_norm": 155.0,
      "learning_rate": 2e-05,
      "loss": 14.1452,
      "step": 246
    },
    {
      "epoch": 0.2008130081300813,
      "grad_norm": 122.0,
      "learning_rate": 2e-05,
      "loss": 14.1099,
      "step": 247
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 128.0,
      "learning_rate": 2e-05,
      "loss": 13.9423,
      "step": 248
    },
    {
      "epoch": 0.20243902439024392,
      "grad_norm": 187.0,
      "learning_rate": 2e-05,
      "loss": 13.9756,
      "step": 249
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 88.0,
      "learning_rate": 2e-05,
      "loss": 14.0372,
      "step": 250
    },
    {
      "epoch": 0.2040650406504065,
      "grad_norm": 156.0,
      "learning_rate": 2e-05,
      "loss": 13.819,
      "step": 251
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 96.5,
      "learning_rate": 2e-05,
      "loss": 13.9221,
      "step": 252
    },
    {
      "epoch": 0.2056910569105691,
      "grad_norm": 145.0,
      "learning_rate": 2e-05,
      "loss": 14.2655,
      "step": 253
    },
    {
      "epoch": 0.20650406504065041,
      "grad_norm": 171.0,
      "learning_rate": 2e-05,
      "loss": 13.9958,
      "step": 254
    },
    {
      "epoch": 0.2073170731707317,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 13.3902,
      "step": 255
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 121.5,
      "learning_rate": 2e-05,
      "loss": 14.1699,
      "step": 256
    },
    {
      "epoch": 0.20894308943089432,
      "grad_norm": 92.0,
      "learning_rate": 2e-05,
      "loss": 13.6965,
      "step": 257
    },
    {
      "epoch": 0.2097560975609756,
      "grad_norm": 124.5,
      "learning_rate": 2e-05,
      "loss": 14.3387,
      "step": 258
    },
    {
      "epoch": 0.2105691056910569,
      "grad_norm": 112.5,
      "learning_rate": 2e-05,
      "loss": 14.4105,
      "step": 259
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 160.0,
      "learning_rate": 2e-05,
      "loss": 14.1108,
      "step": 260
    },
    {
      "epoch": 0.2121951219512195,
      "grad_norm": 134.0,
      "learning_rate": 2e-05,
      "loss": 14.1982,
      "step": 261
    },
    {
      "epoch": 0.21300813008130082,
      "grad_norm": 131.0,
      "learning_rate": 2e-05,
      "loss": 13.835,
      "step": 262
    },
    {
      "epoch": 0.2138211382113821,
      "grad_norm": 112.0,
      "learning_rate": 2e-05,
      "loss": 13.8022,
      "step": 263
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 129.0,
      "learning_rate": 2e-05,
      "loss": 14.1419,
      "step": 264
    },
    {
      "epoch": 0.21544715447154472,
      "grad_norm": 107.5,
      "learning_rate": 2e-05,
      "loss": 14.4291,
      "step": 265
    },
    {
      "epoch": 0.216260162601626,
      "grad_norm": 144.0,
      "learning_rate": 2e-05,
      "loss": 13.4836,
      "step": 266
    },
    {
      "epoch": 0.21707317073170732,
      "grad_norm": 144.0,
      "learning_rate": 2e-05,
      "loss": 14.3341,
      "step": 267
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 113.5,
      "learning_rate": 2e-05,
      "loss": 13.608,
      "step": 268
    },
    {
      "epoch": 0.2186991869918699,
      "grad_norm": 120.5,
      "learning_rate": 2e-05,
      "loss": 13.9608,
      "step": 269
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 109.5,
      "learning_rate": 2e-05,
      "loss": 13.8607,
      "step": 270
    },
    {
      "epoch": 0.22032520325203253,
      "grad_norm": 140.0,
      "learning_rate": 2e-05,
      "loss": 14.0078,
      "step": 271
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 146.0,
      "learning_rate": 2e-05,
      "loss": 13.8646,
      "step": 272
    },
    {
      "epoch": 0.22195121951219512,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 14.4907,
      "step": 273
    },
    {
      "epoch": 0.22276422764227644,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 13.4816,
      "step": 274
    },
    {
      "epoch": 0.22357723577235772,
      "grad_norm": 149.0,
      "learning_rate": 2e-05,
      "loss": 13.3853,
      "step": 275
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 138.0,
      "learning_rate": 2e-05,
      "loss": 13.7841,
      "step": 276
    },
    {
      "epoch": 0.2252032520325203,
      "grad_norm": 131.0,
      "learning_rate": 2e-05,
      "loss": 14.2375,
      "step": 277
    },
    {
      "epoch": 0.22601626016260162,
      "grad_norm": 92.5,
      "learning_rate": 2e-05,
      "loss": 13.7743,
      "step": 278
    },
    {
      "epoch": 0.22682926829268293,
      "grad_norm": 134.0,
      "learning_rate": 2e-05,
      "loss": 13.6378,
      "step": 279
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 96.0,
      "learning_rate": 2e-05,
      "loss": 13.931,
      "step": 280
    },
    {
      "epoch": 0.22845528455284553,
      "grad_norm": 95.0,
      "learning_rate": 2e-05,
      "loss": 13.9197,
      "step": 281
    },
    {
      "epoch": 0.22926829268292684,
      "grad_norm": 120.0,
      "learning_rate": 2e-05,
      "loss": 13.4303,
      "step": 282
    },
    {
      "epoch": 0.23008130081300812,
      "grad_norm": 118.5,
      "learning_rate": 2e-05,
      "loss": 13.5428,
      "step": 283
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 86.5,
      "learning_rate": 2e-05,
      "loss": 13.954,
      "step": 284
    },
    {
      "epoch": 0.23170731707317074,
      "grad_norm": 129.0,
      "learning_rate": 2e-05,
      "loss": 13.6645,
      "step": 285
    },
    {
      "epoch": 0.23252032520325203,
      "grad_norm": 89.0,
      "learning_rate": 2e-05,
      "loss": 13.7735,
      "step": 286
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 123.0,
      "learning_rate": 2e-05,
      "loss": 13.6684,
      "step": 287
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 189.0,
      "learning_rate": 2e-05,
      "loss": 13.3221,
      "step": 288
    },
    {
      "epoch": 0.23495934959349593,
      "grad_norm": 121.5,
      "learning_rate": 2e-05,
      "loss": 14.1726,
      "step": 289
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 156.0,
      "learning_rate": 2e-05,
      "loss": 13.8182,
      "step": 290
    },
    {
      "epoch": 0.23658536585365852,
      "grad_norm": 140.0,
      "learning_rate": 2e-05,
      "loss": 13.4642,
      "step": 291
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 126.5,
      "learning_rate": 2e-05,
      "loss": 13.5949,
      "step": 292
    },
    {
      "epoch": 0.23821138211382115,
      "grad_norm": 111.0,
      "learning_rate": 2e-05,
      "loss": 13.5687,
      "step": 293
    },
    {
      "epoch": 0.23902439024390243,
      "grad_norm": 89.0,
      "learning_rate": 2e-05,
      "loss": 13.6177,
      "step": 294
    },
    {
      "epoch": 0.23983739837398374,
      "grad_norm": 169.0,
      "learning_rate": 2e-05,
      "loss": 12.8704,
      "step": 295
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 203.0,
      "learning_rate": 2e-05,
      "loss": 13.2518,
      "step": 296
    },
    {
      "epoch": 0.24146341463414633,
      "grad_norm": 121.5,
      "learning_rate": 2e-05,
      "loss": 13.7501,
      "step": 297
    },
    {
      "epoch": 0.24227642276422764,
      "grad_norm": 113.5,
      "learning_rate": 2e-05,
      "loss": 13.8258,
      "step": 298
    },
    {
      "epoch": 0.24308943089430896,
      "grad_norm": 120.5,
      "learning_rate": 2e-05,
      "loss": 13.8449,
      "step": 299
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 102.5,
      "learning_rate": 2e-05,
      "loss": 13.9022,
      "step": 300
    },
    {
      "epoch": 0.24471544715447155,
      "grad_norm": 108.5,
      "learning_rate": 2e-05,
      "loss": 13.9235,
      "step": 301
    },
    {
      "epoch": 0.24552845528455283,
      "grad_norm": 512.0,
      "learning_rate": 2e-05,
      "loss": 15.6439,
      "step": 302
    },
    {
      "epoch": 0.24634146341463414,
      "grad_norm": 516.0,
      "learning_rate": 2e-05,
      "loss": 15.0733,
      "step": 303
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 520.0,
      "learning_rate": 2e-05,
      "loss": 14.8944,
      "step": 304
    },
    {
      "epoch": 0.24796747967479674,
      "grad_norm": 496.0,
      "learning_rate": 2e-05,
      "loss": 14.6135,
      "step": 305
    },
    {
      "epoch": 0.24878048780487805,
      "grad_norm": 532.0,
      "learning_rate": 2e-05,
      "loss": 15.4172,
      "step": 306
    },
    {
      "epoch": 0.24959349593495936,
      "grad_norm": 502.0,
      "learning_rate": 2e-05,
      "loss": 15.0231,
      "step": 307
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 472.0,
      "learning_rate": 2e-05,
      "loss": 14.6892,
      "step": 308
    },
    {
      "epoch": 0.25121951219512195,
      "grad_norm": 464.0,
      "learning_rate": 2e-05,
      "loss": 14.6892,
      "step": 309
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 462.0,
      "learning_rate": 2e-05,
      "loss": 14.4295,
      "step": 310
    },
    {
      "epoch": 0.2528455284552846,
      "grad_norm": 474.0,
      "learning_rate": 2e-05,
      "loss": 13.7562,
      "step": 311
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 420.0,
      "learning_rate": 2e-05,
      "loss": 13.9629,
      "step": 312
    },
    {
      "epoch": 0.25447154471544714,
      "grad_norm": 392.0,
      "learning_rate": 2e-05,
      "loss": 13.8977,
      "step": 313
    },
    {
      "epoch": 0.2552845528455285,
      "grad_norm": 324.0,
      "learning_rate": 2e-05,
      "loss": 13.5251,
      "step": 314
    },
    {
      "epoch": 0.25609756097560976,
      "grad_norm": 330.0,
      "learning_rate": 2e-05,
      "loss": 13.2525,
      "step": 315
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 362.0,
      "learning_rate": 2e-05,
      "loss": 14.1343,
      "step": 316
    },
    {
      "epoch": 0.2577235772357724,
      "grad_norm": 352.0,
      "learning_rate": 2e-05,
      "loss": 14.0442,
      "step": 317
    },
    {
      "epoch": 0.25853658536585367,
      "grad_norm": 262.0,
      "learning_rate": 2e-05,
      "loss": 13.484,
      "step": 318
    },
    {
      "epoch": 0.25934959349593495,
      "grad_norm": 218.0,
      "learning_rate": 2e-05,
      "loss": 13.329,
      "step": 319
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 218.0,
      "learning_rate": 2e-05,
      "loss": 13.1212,
      "step": 320
    },
    {
      "epoch": 0.26097560975609757,
      "grad_norm": 161.0,
      "learning_rate": 2e-05,
      "loss": 13.4032,
      "step": 321
    },
    {
      "epoch": 0.26178861788617885,
      "grad_norm": 159.0,
      "learning_rate": 2e-05,
      "loss": 13.4277,
      "step": 322
    },
    {
      "epoch": 0.26260162601626014,
      "grad_norm": 137.0,
      "learning_rate": 2e-05,
      "loss": 13.2849,
      "step": 323
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 130.0,
      "learning_rate": 2e-05,
      "loss": 13.0578,
      "step": 324
    },
    {
      "epoch": 0.26422764227642276,
      "grad_norm": 116.0,
      "learning_rate": 2e-05,
      "loss": 13.2129,
      "step": 325
    },
    {
      "epoch": 0.26504065040650404,
      "grad_norm": 131.0,
      "learning_rate": 2e-05,
      "loss": 13.1792,
      "step": 326
    },
    {
      "epoch": 0.2658536585365854,
      "grad_norm": 116.0,
      "learning_rate": 2e-05,
      "loss": 12.9914,
      "step": 327
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 13.4014,
      "step": 328
    },
    {
      "epoch": 0.26747967479674795,
      "grad_norm": 154.0,
      "learning_rate": 2e-05,
      "loss": 12.9981,
      "step": 329
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 138.0,
      "learning_rate": 2e-05,
      "loss": 13.029,
      "step": 330
    },
    {
      "epoch": 0.26910569105691057,
      "grad_norm": 157.0,
      "learning_rate": 2e-05,
      "loss": 13.1113,
      "step": 331
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 168.0,
      "learning_rate": 2e-05,
      "loss": 13.4721,
      "step": 332
    },
    {
      "epoch": 0.2707317073170732,
      "grad_norm": 148.0,
      "learning_rate": 2e-05,
      "loss": 13.1277,
      "step": 333
    },
    {
      "epoch": 0.27154471544715447,
      "grad_norm": 113.5,
      "learning_rate": 2e-05,
      "loss": 13.1394,
      "step": 334
    },
    {
      "epoch": 0.27235772357723576,
      "grad_norm": 169.0,
      "learning_rate": 2e-05,
      "loss": 13.3161,
      "step": 335
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 139.0,
      "learning_rate": 2e-05,
      "loss": 13.4032,
      "step": 336
    },
    {
      "epoch": 0.2739837398373984,
      "grad_norm": 147.0,
      "learning_rate": 2e-05,
      "loss": 13.5028,
      "step": 337
    },
    {
      "epoch": 0.27479674796747966,
      "grad_norm": 161.0,
      "learning_rate": 2e-05,
      "loss": 13.358,
      "step": 338
    },
    {
      "epoch": 0.275609756097561,
      "grad_norm": 126.0,
      "learning_rate": 2e-05,
      "loss": 13.2997,
      "step": 339
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 89.5,
      "learning_rate": 2e-05,
      "loss": 13.3674,
      "step": 340
    },
    {
      "epoch": 0.27723577235772356,
      "grad_norm": 126.0,
      "learning_rate": 2e-05,
      "loss": 13.5139,
      "step": 341
    },
    {
      "epoch": 0.2780487804878049,
      "grad_norm": 195.0,
      "learning_rate": 2e-05,
      "loss": 12.8708,
      "step": 342
    },
    {
      "epoch": 0.2788617886178862,
      "grad_norm": 92.0,
      "learning_rate": 2e-05,
      "loss": 12.9923,
      "step": 343
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 129.0,
      "learning_rate": 2e-05,
      "loss": 13.5631,
      "step": 344
    },
    {
      "epoch": 0.2804878048780488,
      "grad_norm": 123.5,
      "learning_rate": 2e-05,
      "loss": 12.9857,
      "step": 345
    },
    {
      "epoch": 0.2813008130081301,
      "grad_norm": 133.0,
      "learning_rate": 2e-05,
      "loss": 13.4555,
      "step": 346
    },
    {
      "epoch": 0.2821138211382114,
      "grad_norm": 175.0,
      "learning_rate": 2e-05,
      "loss": 12.4206,
      "step": 347
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 151.0,
      "learning_rate": 2e-05,
      "loss": 12.9054,
      "step": 348
    },
    {
      "epoch": 0.283739837398374,
      "grad_norm": 98.5,
      "learning_rate": 2e-05,
      "loss": 12.6016,
      "step": 349
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 108.5,
      "learning_rate": 2e-05,
      "loss": 13.0432,
      "step": 350
    },
    {
      "epoch": 0.28536585365853656,
      "grad_norm": 104.0,
      "learning_rate": 2e-05,
      "loss": 13.0844,
      "step": 351
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 98.0,
      "learning_rate": 2e-05,
      "loss": 13.1663,
      "step": 352
    },
    {
      "epoch": 0.2869918699186992,
      "grad_norm": 144.0,
      "learning_rate": 2e-05,
      "loss": 12.6651,
      "step": 353
    },
    {
      "epoch": 0.28780487804878047,
      "grad_norm": 84.5,
      "learning_rate": 2e-05,
      "loss": 12.7172,
      "step": 354
    },
    {
      "epoch": 0.2886178861788618,
      "grad_norm": 96.0,
      "learning_rate": 2e-05,
      "loss": 13.2972,
      "step": 355
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 101.5,
      "learning_rate": 2e-05,
      "loss": 13.1694,
      "step": 356
    },
    {
      "epoch": 0.29024390243902437,
      "grad_norm": 148.0,
      "learning_rate": 2e-05,
      "loss": 13.526,
      "step": 357
    },
    {
      "epoch": 0.2910569105691057,
      "grad_norm": 112.0,
      "learning_rate": 2e-05,
      "loss": 12.5621,
      "step": 358
    },
    {
      "epoch": 0.291869918699187,
      "grad_norm": 100.5,
      "learning_rate": 2e-05,
      "loss": 12.7231,
      "step": 359
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 143.0,
      "learning_rate": 2e-05,
      "loss": 12.6955,
      "step": 360
    },
    {
      "epoch": 0.2934959349593496,
      "grad_norm": 202.0,
      "learning_rate": 2e-05,
      "loss": 13.0254,
      "step": 361
    },
    {
      "epoch": 0.2943089430894309,
      "grad_norm": 94.5,
      "learning_rate": 2e-05,
      "loss": 13.0187,
      "step": 362
    },
    {
      "epoch": 0.2951219512195122,
      "grad_norm": 202.0,
      "learning_rate": 2e-05,
      "loss": 13.6302,
      "step": 363
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 98.5,
      "learning_rate": 2e-05,
      "loss": 12.8609,
      "step": 364
    },
    {
      "epoch": 0.2967479674796748,
      "grad_norm": 162.0,
      "learning_rate": 2e-05,
      "loss": 13.0634,
      "step": 365
    },
    {
      "epoch": 0.2975609756097561,
      "grad_norm": 107.0,
      "learning_rate": 2e-05,
      "loss": 12.4943,
      "step": 366
    },
    {
      "epoch": 0.2983739837398374,
      "grad_norm": 92.5,
      "learning_rate": 2e-05,
      "loss": 12.7717,
      "step": 367
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 116.5,
      "learning_rate": 2e-05,
      "loss": 12.9823,
      "step": 368
    },
    {
      "epoch": 0.3,
      "grad_norm": 139.0,
      "learning_rate": 2e-05,
      "loss": 12.4811,
      "step": 369
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 125.5,
      "learning_rate": 2e-05,
      "loss": 12.6733,
      "step": 370
    },
    {
      "epoch": 0.3016260162601626,
      "grad_norm": 135.0,
      "learning_rate": 2e-05,
      "loss": 12.3028,
      "step": 371
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 91.5,
      "learning_rate": 2e-05,
      "loss": 13.2325,
      "step": 372
    },
    {
      "epoch": 0.3032520325203252,
      "grad_norm": 106.0,
      "learning_rate": 2e-05,
      "loss": 12.7682,
      "step": 373
    },
    {
      "epoch": 0.3040650406504065,
      "grad_norm": 107.0,
      "learning_rate": 2e-05,
      "loss": 12.4971,
      "step": 374
    },
    {
      "epoch": 0.3048780487804878,
      "grad_norm": 156.0,
      "learning_rate": 2e-05,
      "loss": 12.9237,
      "step": 375
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 104.0,
      "learning_rate": 2e-05,
      "loss": 12.9501,
      "step": 376
    },
    {
      "epoch": 0.3065040650406504,
      "grad_norm": 125.5,
      "learning_rate": 2e-05,
      "loss": 12.6574,
      "step": 377
    },
    {
      "epoch": 0.3073170731707317,
      "grad_norm": 103.0,
      "learning_rate": 2e-05,
      "loss": 11.9589,
      "step": 378
    },
    {
      "epoch": 0.308130081300813,
      "grad_norm": 110.0,
      "learning_rate": 2e-05,
      "loss": 13.4396,
      "step": 379
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 126.5,
      "learning_rate": 2e-05,
      "loss": 13.0984,
      "step": 380
    },
    {
      "epoch": 0.3097560975609756,
      "grad_norm": 102.0,
      "learning_rate": 2e-05,
      "loss": 12.937,
      "step": 381
    },
    {
      "epoch": 0.3105691056910569,
      "grad_norm": 111.0,
      "learning_rate": 2e-05,
      "loss": 12.9738,
      "step": 382
    },
    {
      "epoch": 0.31138211382113823,
      "grad_norm": 119.0,
      "learning_rate": 2e-05,
      "loss": 12.5052,
      "step": 383
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 108.0,
      "learning_rate": 2e-05,
      "loss": 12.6142,
      "step": 384
    },
    {
      "epoch": 0.3130081300813008,
      "grad_norm": 107.0,
      "learning_rate": 2e-05,
      "loss": 12.498,
      "step": 385
    },
    {
      "epoch": 0.31382113821138213,
      "grad_norm": 102.0,
      "learning_rate": 2e-05,
      "loss": 12.2976,
      "step": 386
    },
    {
      "epoch": 0.3146341463414634,
      "grad_norm": 112.5,
      "learning_rate": 2e-05,
      "loss": 13.0488,
      "step": 387
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 106.5,
      "learning_rate": 2e-05,
      "loss": 12.3123,
      "step": 388
    },
    {
      "epoch": 0.31626016260162604,
      "grad_norm": 94.5,
      "learning_rate": 2e-05,
      "loss": 12.9329,
      "step": 389
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 106.5,
      "learning_rate": 2e-05,
      "loss": 12.8202,
      "step": 390
    },
    {
      "epoch": 0.3178861788617886,
      "grad_norm": 96.5,
      "learning_rate": 2e-05,
      "loss": 13.2353,
      "step": 391
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 119.5,
      "learning_rate": 2e-05,
      "loss": 12.6757,
      "step": 392
    },
    {
      "epoch": 0.3195121951219512,
      "grad_norm": 98.0,
      "learning_rate": 2e-05,
      "loss": 12.7693,
      "step": 393
    },
    {
      "epoch": 0.3203252032520325,
      "grad_norm": 118.0,
      "learning_rate": 2e-05,
      "loss": 12.6877,
      "step": 394
    },
    {
      "epoch": 0.32113821138211385,
      "grad_norm": 138.0,
      "learning_rate": 2e-05,
      "loss": 13.6222,
      "step": 395
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 151.0,
      "learning_rate": 2e-05,
      "loss": 12.3545,
      "step": 396
    },
    {
      "epoch": 0.3227642276422764,
      "grad_norm": 109.0,
      "learning_rate": 2e-05,
      "loss": 13.0212,
      "step": 397
    },
    {
      "epoch": 0.3235772357723577,
      "grad_norm": 95.5,
      "learning_rate": 2e-05,
      "loss": 12.9367,
      "step": 398
    },
    {
      "epoch": 0.32439024390243903,
      "grad_norm": 164.0,
      "learning_rate": 2e-05,
      "loss": 12.8779,
      "step": 399
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 118.5,
      "learning_rate": 2e-05,
      "loss": 12.6196,
      "step": 400
    },
    {
      "epoch": 0.3252032520325203,
      "step": 400,
      "total_flos": 2.669735338573824e+17,
      "train_loss": 17.76908321380615,
      "train_runtime": 2724.7401,
      "train_samples_per_second": 0.147,
      "train_steps_per_second": 0.147
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.669735338573824e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
