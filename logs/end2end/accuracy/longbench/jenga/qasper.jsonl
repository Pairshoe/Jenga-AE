{"pred": " The ground truth for fake news is established by the expert who annotated the dataset of tweets as containing fake news or not containing them. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset. Out of those 137 tweets that went viral, 136 were annotated as containing fake news (according to the categories previously described), and the rest were classified as \"non containing fake news\". The categorization is far from being perfect given the ambiguity of fake news themselves and human judgement involved in the process of categorization. Because of this,", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " GhostVAD is an extension of the NetVAD approach proposed for face recognition by R. Arandjelovic et al. [1]. The GhostVAD is inspired by the NetVAD approach where they were able to replace the hard assignment based clustering with soft assignment based clustering so that it can be trained with neural network in an end to end. In our case, we use the GhostVAD layer to map N local features of dimension D into a fixed dimensional vector.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " The model outperforms the state of the art results by 68.8% to 71.8% when applied to the IEMOC dataset, which is one of the most well-studied datasets.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " The additional features and context proposed in the article include:\n\n- Looking at the tweet that was replied to or quoted provides significant contextual information, which humans can better understand the tweet's meaning. This is called \"context tweets\".\n\n- Using the context tweet that the target of the abusive tweet is replying to as additional information in classifying the text data.\n\n- Using the last hidden states of context and labeled tweets as additional features for the RNN model.\n- Using the last hidden states of context and labeled tweets as additional features for the CNN model", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at Facebook pages like FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " The hashtag dataset contains 1,108 unique hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset. The SemEval dataset contains 12,128 tweets with 12,128 hashtags. The SemEval dataset contains 12,28 tweets with 12,128 hashtags. So both datasets contain English data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The proposed evaluation for this task is based on the peer scores of the 58 summaries in TAC208, which are used to calculate a score as the sum of the importance of the propositions they contain. This score correlates with the manual responsiveness scores assigned during TAC in comparison to ROUGE-2 and Pyramid scores.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The question is based on the article as follows:\n\n\nThe summarization datasets used in our experiments and discussed various implementation details are:\n\n- CNN/Daily news highlights dataset BREF24, the New York Times Annotated Corpus (NYT; BREF25), and XSum BREF2. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive).", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The approach proposed in the article compares to other WSD approaches employing word embeddings by:\n\n- Using KL divergence as the energy function to capture textual entailment between words, which captures polysemous nature of words and reduces uncertainty per word by distributing it across senses.\n\n- Capturing polysemy by providing apriori heuristics about senses in the dataset\n- Handling words with multiple meanings (polysemies) by a modified skip-gram model and EM algorithm\n- Using a neural approach considering both local and global contexts in learning", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " The ensemble method works by taking the best performing model from the validation set and adding it to an existing ensemble of models. This model is then evaluated on the BookTest dataset to see if it can answer the questions. If it can, it is added to the ensemble. If not, it is discarded. This process is repeated until all models have been tried. The final ensemble is the best performing model from the BookTest validation set.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The article states that the EmotionLines dataset consists of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom, while the latter is made up of Facebook messenger chats. Each subset includes $1,000$ English dialogues and each dialogue can be further divided into a few consecutive utterances. The utterances are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. Annotator", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The paper focuses on using neural text simplification (NTS) systems to simplify text for non-native speakers, children, and people with cognitive disabilities. Specifically, it proposes using simplified corpora during training of neural text simplification (NTS) systems to improve their performance.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The IMDb dataset used for sentiment analysis is the IMDb dataset of movie reviews by BREF1 that contains 500 sentences with 200 half being positive sentiments and the other half being negative sentiments. The IMDb dataset has a total of 25,00 sentences with 7 labels and 9 main labels and 2 contexts. It is unbalanced due to the high percentage of tokens with the label 'O'.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " The proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ. This indicates that LSTM-CRF is a very strong baseline system, demonstrating the effectiveness of neural network.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes, they experimented with this new dataset. The article states:\n\n\n\"In this work, we focus on eye-tracking and electroencephalography (EEG) recordings to capture the reading process. On one hand, eye movement provides millisecond-acate records about humans look when they are reading, and is highly correlated with the cognitive load associated with different stages of text processing. On the other hand, EEG records electrical brain activity across the scalp and a direct measure of physiological processes, including language processing. The combination of both measurement methods enables us to study the language understanding", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The article does not mention any datasets used for training the intent classifier. It only mentions that the intentions and actions are extracted from user utterances. The document states that the intentions and actions are extracted from user utterances.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The Healthcare sector achieved the best performance, with an $R^2$ score of 0.4.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " They compared the NMT models with the SMT models on the dataset they built. Specifically, they compared the RNN-based NMT model, Transformer-NMT model, and SMT model on the dataset they built.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are:\n\n1. A regularization term associated with neutral features (neutral features)\n\n2. The maximum entropy of class distribution regularization term\n3. The KL divergence between reference and predicted distribution", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The baselines are:\n\n1) SVM with unigram, bigram, and trigram features, which is a standard yet strong classifier for text features;\n\n2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words;\n3) SVM with average transformed word embeddings (the INLINEFORM0 equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words;\n4) two deep learning models on text classification, CNNIBREF", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " They improved the performance by 1.2 points on the fine-grained sentiment classification task.\n\n\nThe passage states:\n\n\"The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BREF2 , which to the best of our knowledge holds the state-of-the-art. Due to the stochasticity of training the biLSTM models, we repeat the experiment 10 times and report the average and the standard deviation of the performance. Several observations", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The model introduces adaptive attention, allowing the shape and sparsity of the attention to be learned automatically. This is particularly appealing in the context of multi-head attention mechanisms, where different heads learn different sparsity behaviors. The adaptive model has an additional scalar parameter per attention per layer for each attention mechanism (encoder self-attention, context, and decoder self-attention), i.e., and sets $\\alpha_{i,j}^t = 1 + \\operatornamewithlimits{\\sigma}(a_{i,j}^t) \\in [1,2[$. All of", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model used in the article is the Transformer base model. Specifically, the number of layers is $N=6$, with $h=8$ parallel attention layers, dimensionality of input and output is $d_{model}=512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=204$.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " The article does not provide any metrics for evaluation. The question is unanswerable.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is task-specific, three attentions are defined. Specifically, the size of $\\mathcal {A}$ and $\\mathcal {M}$ is much larger than $\\mathcal {S}$. Therefore, the common practice is to pre-train the model on ASR and MT tasks and then fine-tune it with a multi-task learning manner. However, as mentioned in the article, this method suffers from subnet waste, role mismatch and non-pretrained attention issues, which severely limits the end-to-end ST performance.\n\n\nSo the attention module is pretrained on the", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " The kind of stylistic features obtained are:\n\n- Unigrams and Pragmatic features\n- BREF0 , BIBREF1 , BIBREF2 , BIBREF3 (b) Stylistic patterns BREF4 and related to situational disparity BREF5 and (c) Hastag interpretations BREF6 , BREF7\n\n\n- Linguistic features like emoticons, laughter expressions “lol” etc.\n- Distant supervision based techniques (leveraging hashtags) and stylistic/pragmatic features\n-", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The encoder has an LSTM encoder that encodes the entire context and an LSTM decoder that generates the word form.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes, WordNet is useful for taxonomic reasoning for this task because it provides a comprehensive lexical ontology of English words and their definitions, synsets, and example sentences. It allows us to construct natural language questions that contextualize the types of concepts we want to probe. Specifically, WordNet allows us to construct natural language questions that contextualize the types of concepts we want to probe, such as definitions, hypernymy, synonymy, and word sense disambiguation.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baselines mentioned in the article are:\n\n- Conventional automatic speech recognition (ASR) systems typically consist of several independently learned components: an acoustic model to predict context-dependent subphoneme states (senones), a structure to map senones to phonemes, a pronunciation model to map phonemes, hybrid systems hidden Markov models to model dependencies with neural networks, etc.\n- Newer approaches such as end-to-end (E) systems reduce the overall complexity of the final system.\n- Our research builds on prior work that explored using time-delay neural", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": " The article does not provide any information about how many users look at the article. So the question is unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The article mentions several metrics for evaluation, including:\n\n- BLEU-1/4 and ROUGE-L for quality metrics\n- BLEU-1/4 capture structural information via n-gram matching, but are not correlated with subjective recipe quality\n- BLEU and ROUGE are not the most appropriate metrics for generation quality\n- BLEU and ROUGE are not the most appropriate metrics for generation\n- Distinct-1/2: percentage of distinct unigrams and bigrams and acceptable recipes\n- BLEU and ROUGE are not", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " They create the following labels on their dataset:\n\n- Open-ended Inquiry: Inquiries about well-being or a particular symptom; e.g., “How are you feeling?” and “Do you cough?”\n\n- Detailed Inquiry: Inquiries with specific details that prompt yes/no answers or clarifications; e.g., “Do you cough at night?”\n- Multi-Intent Inquiry: Inquiring more than one symptom in a question; e.g., “Any cough, chest pain, or headache?”\n- Re", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The article does not provide any information about the amount of data needed to train the task-specific encoder. It only discusses the amount of data needed to train the universal sentence encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " The article evaluates the adaptively sparse Transformer on four machine translation tasks:\n\n- IWSLT 2017 German-English\n-English\n- KFT Japanese-English\n- English\n- WMT 2016 Romanian-English\n-German\n- English\n\nThe document provides test set results for each task in Table 27. It shows that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for Estonian in the NER task is shown in Table TABREF21. The results for all languages and all ELMo layers, averaged over semantic and syntactic categories, are shown in Table TABREF17. The embeddings after the first LSTM layer perform best in semantic categories. In syntactic categories, the non-contextual CNN performs the best. Syntactic categories are less context dependent and much more morphology and syntax based, so it is not surprising that the non-contextual layer performs well. The second LSTM layer embeddings perform", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " They have a background in the humanities and social sciences, working with sources that are not available in digital form and often work with both analogue and digitized materials. They also work with multiple data sources. The variety of sources typically used means that more than one data collection method is often required. For example, a project examining coverage of a UK General Election, could draw data from traditional media, web archives, Twitter, Facebook, manifestos, etc. and combine textual analysis of these materials with surveys, laboratory experiments, or field observations offline.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " No, the paper introduces a supervised approach to spam detection. The LDA model is unsupervised, but the topic-based features are supervised. The paper introduces a supervised approach to spam detection by using the topic-based features.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni, Xho, Sotho, Sotho, Tshivenda, Xitsa and Xitsonga are similar to each other and harder to distinguish.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " They compared 6-layers and 9-layers sMBR models with Xavier initialization. They found that 6-layers sMBR model converges well, but there is no further improvement with increasing the number of layers. So they chose 8-layers sMBR model as the teacher model instead of CE model.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The data set consists of articles from English Wikipedia, with quality labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes in descending order of quality: Featured Article (FA), Good Article (GA), B-class Article (B), C-class Article (C), Start Article (Start), and Stub Article (Stub). A description of the criteria associated with the different classes can be found in the grading scheme page. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human judgements were assembled by a group of 50 native speakers of both English and Tamil languages who were well-versed in both languages. A collection of about 10 sentences from the test set were taken for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy ratings for the RNNMorph results were tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes, they test their framework on commonly used language pairs such as English-to-German. Specifically, they evaluate their performance on the English-German translation task in the under-resourced and zero-resourced translation scenarios. They show that their framework achieves significant improvements over the baseline NMT system in both cases.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " The models are evaluated by training an autocomplete system on 50K randomly sampled sentences from Yelp reviews, quantifying the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. Examples of the autocomplete task and actual user-provided keywords are shown in Table TABREF13. The top three suggestions from the autocomplete system are", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " The evaluation metrics looked at for classification tasks include precision, recall, F1-measure, and F-score. These are computed as follows:\n\n\nINLINEFORM0 would be undefined if INLINEFORM1 is empty and similarly INLINEFORM2 would be undefined when INLINEFORM3 is empty. Hence, overall precision and recall are computed by averaging over all the instances except where they are undefined. Instance-level Fmeasure can not be computed for instances where either precision or recall are undefined. Therefore, overall F-measure is computed using the overall precision and recall.\n\nSo the evaluation metrics looked at are", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the domain with labeled data, and the target domain is the domain with unlabeled data.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " They compare the PRU with state-of-the-art models including:\n\n\n- AWD-LSTM (AWD-LSTM) - a language model with LSTM layers\n-based architecture\n- BIBREF28 - prepared by BIBREF29\n- Penn Treebank (PTB) dataset\n- BREF20 - WikiText2 (WT-2) dataset\n\nSo the previous RNN models they compare with are:\n- AWD-LSTM (AWD-LSTM) - a language model with LSTM", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " NeuronBlocks includes the following neural network modules:\n\n- Embedding Layer: Word/character embedding and extra handcrafted feature embedding such as pos-tagging\n- Neural Network Layers: Block zoo provides common layers like RNN, CNN, QRNN, Transformer, Encoder architecture, etc.\n- Attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention, Bidirectional attention, etc.\n- Loss Function: Besides the loss functions built in PyT", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " They used the multilingual pronunciation corpus collected by deri016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10.\n\n\n\nQuestion: what metrics did they use to evaluate the models?\n\nAnswer: They used the following evaluation metrics:\n\n\nPhoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The baselines used in the paper are:\n\n- BERT (BIBREF12) - bert-base-uncaseds3.amazonaws.com/models.huggingface.co/bert-base-uncased.tar.gz (The model used by BIBREF12)\n\n- RoBERTa (BREF2) - roberta-bases3.amazonaws.com/models.huggingface.co/roberta-pych_model.bin (RoBERTa-base does not have an uncased variant)\n-", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " They use English, Spanish, Finnish, and Spanish in their experiment.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
