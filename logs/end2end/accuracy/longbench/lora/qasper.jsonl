{"pred": " The ground truth for fake news is not established in the article. The article does not provide any information about how to establish the ground truth for fake news. The article focuses on analyzing the meta-data of tweets that went viral during the 2016 US presidential election to identify features that may be characteristic of fake news. The article does not discuss how to determine whether a tweet is fake or not.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " GhostVAD is an extension of the NetVAD approach proposed for face recognition by R. Arjelovic et al. [1]. The GhostVAD is inspired by the NetVAD approach for face recognition, where they were able to replace the hard assignment based clustering with soft assignment based clustering so that it can be trained with neural network in an end to end. In our case, we use the GhostVAD layer to map N dimensional local features of dimension D into a fixed dimensional vector.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " The model outperforms the state of the art results by 6.8% to 7.8% when applied to the IEMOC dataset.\n\n\nThe article states: \"In particular, the speech recognition task is one of the most important problems in the field of paralistics. This has recently broadened applications as it is a crucial factor in optimal human-computer interactions.\"\n\nSo the question is: \"How does the speech recognition task contribute to human-computer interactions?\"\n\n\nThe answer is: \"The speech recognition task contributes to human-computer interactions by predict", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " The additional features and context proposed in the article are:\n\n- Using context tweets as additional features for neural network models\n- Using context tweets as additional features for neural network models\n\n- Using the same baseline model to convert context tweets to vectors, then concatenate these vectors with outputs of their corresponding labeled tweets\n- Concatenating max-poled layers of context and labeled tweets for the CNN baseline model\n- Concatenating last hidden states of context and labeled tweets for the RNN model\n- Using the last hidden states of context and labeled tweets as additional", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at Facebook pages like FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\n\nQuestion: What is the name of the dataset they used for the development set?\nAnswer: Affective Text dataset\nQuestion: What is the name of the dataset they used for the evaluation set?\nAnswer: ISEAR dataset\n\nQuestion: What is the name of the dataset they used for the evaluation set?\nAnswer", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " The article does not mention the language of the hashtag and SemEval datasets. The hashtag dataset is English, but the SemEval dataset does not specify the language.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The proposed evaluation for this task is based on the manual annotations of the concept maps created by experts. The corpus contains 30 concept maps for 30 topics, each with 40 documents. The manual annotations are used to calculate the Pearson correlation between the importance scores of the 58 peer summaries in TAC08 and the manual summaries assigned by the ROUGE-2 and Pyramid systems. The manual annotations are used to calculate the correlation between the importance scores of the propositions in the concept maps and the manual summaries. The proposed evaluation is to compare the importance annotations with the", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The question is based on the article as follows:\n\n\"We evaluated our model on three benchmark datasets, namely the CNN/Daily Mail highlights dataset BREF24, the New York Times Annotated Corpus (NYT; BREF25), and XSum BREF2. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table 12 presents statistics on", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The approach proposed in the article, GM$KL, compares favorably to other WSD approaches in the following ways:\n\n1. It captures polysemous words by using a variant of max-margin objective function on asymmetric KL divergence.\n\n2. It captures word similarity using a Gaussian mixture model with expected likelihood kernel.\n\n3. It handles multiple senses by providing apriori heuristics about senses in the dataset.\n\n4. It handles polysemous words by a modified skip-gram model and EM algorithm.\n\n5. It", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " The ensemble method works by taking the best performing model from the validation set and adding it to the existing ensemble. They then try adding the best performing model that has not been tried yet. If it improves the validation performance, it is added to the ensemble. If it does not, it is discarded. This process is repeated until all models have been tried.\n\n\nThe final ensemble is formed by averaging the predictions from the constituent models. The best performing model is selected from the validation set and added to the ensemble. The best performing model is then added to the ensemble. If it improves the validation performance, it", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The article states that the datasets are:\n\n- Friends dataset: speech-based dialogues from the Friends TV show\n\n- EmotionPush dataset: chat-based dialogues from Facebook messenger chats\n\n\nSo the sources of the datasets are different, with Friends being speech-based and EmotionPush being chat-based.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The paper focuses on using neural text simplification (NTS) systems to improve the fluency of text. Specifically, it proposes using synthetic data from simplified corpora during training to improve the fluency of neural text simplification (NTS) systems.\n\n\nQuestion: What is the main challenge of lexical simplification approaches?\nAnswer: The main challenge is that a large number of transformation rules are required for reasonable coverage and the syntax and semantic meaning of the sentence is hard to retain.\n\nQuestion: What are the three categories of automatic text simplification approaches?\nAnswer: Lexical simplification (", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The sentiment analysis dataset used is IMDb movie reviews by BREF1 that contains 500 sentences with 2000 half being positive sentiments and the other half being negative sentiments.\n\n\nQuestion: What is the objective of this work?\nAnswer: The objective of this work is to determine the optimal combinations of word2vec hyperparameters for intrinsic evaluation (semantic and syntactic analogies) and extrinsic evaluation (NER and SA) tasks.\n\nQuestion: What is the scope of this work?\nAnswer: The scope of this work is to determine the optimal combinations of word2vec", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " The proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ. This indicates that LSTM-CR is a very strong baseline system, demonstrating the effectiveness of neural network.\n\n\nThe proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ. This indicates that LSTM-CR is a very strong baseline system, demonstrating", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes, they did experiment with this new dataset. The article states:\n\n\"In this work, we focus on eye-tracking and electroencephalography (EEG) recordings to capture the reading process. On one hand, eye movement provides millisecond-acate records about humans look when they are reading, and is highly correlated with the cognitive load associated with different stages of text processing. On the other hand, EEG records electrical brain activity across the scalp and a direct measure of physiological processes, including language processing.\"\n\n\nSo they did experiment with the new dataset of simultaneous", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The article does not mention any datasets used for training the intent classifier. It only mentions that the intentions were collected from a set of 14 questions that were asked by users. The authors used the incremental approach to create their own training set for the intent classifier. They then applied the Wizard of Oz method and collected a set of 124 questions that were asked by users. After these questions were manually classified into a set of intentions, they used them to train the first version of the system. The training set had 37 classes and 415 samples per class.\n\n", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The healthcare sector achieved the best performance, with a $R^2$ of 0.4.\n\nThe healthcare sector achieved the best performance, with a $R^2$ of 0.4.\n\nThe healthcare sector achieved the best performance, with a $R^2$ of 0.4.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " They compared the NMT models with the SMT models on the dataset they built. Specifically, they compared the RNN-based NMT model, Transformer-NMT model, and SMT model with Moses toolkit and KenLM.\n\n\nQuestion: what is the best setting for the proposed method?\nAnswer: The best setting involves all three factors: lexical matching, statistical information, and edit distance. They found that the lexical matching is the most important among these factors, and statistical information is more important than edit distance.\n\n\nQuestion: what is the performance of the proposed method compared to previous", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are:\n\n1. A regularization term associated with neutral features (neutral features)\n\n2. The maximum entropy of class distribution regularization term\n3. KL divergence between reference and predicted distribution\n\nThe regularization terms are:\n\n1. A regularization term associated with neutral features (neutral features)\n2. The maximum entropy of class distribution regularization term\n3. KL divergence between reference and predicted distribution", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The baselines are:\n\n1) SVM with unigram, bigram, and trigram features, which is a standard yet strong classifier for text features;\n\n2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of composite words;\n3) SVM with average transformed word embeddings (the INLINEFORM0 equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of composite words;\n4) two deep learning models on text classification, CNN and Recurrent Con", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " The article does not provide any information to answer the question. It does not provide any information about how much they improved. The question is not answeredable.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The article states that their model introduces adaptive sparse attention, which allows the shape and sparsity of the attention to be learned automatically. This is the first neural network that can automatically, continuously vary in shape away from softmax and toward sparse mappings like sparsemax. The adaptively sparse Transformer can dynamically select a sparsity pattern that finds relevant words regardless of their position.\n\n\nTheir adaptive strategy relies only on gradient-based optimization, side-stepping costly per-head hyper-parameter searches. Further speed-ups are possible by leveraging more parallelism in the b", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model used in the article is the Transformer model. The DocRepair model is a monolingual Docair model that corrects inconsistencies between sentence-level translations of a context-agnostic MT system. It does not use any states of a trained MT model whose outputs it corrects and therefore can be trained to correct translations from any black-box MT system.\n\n\nQuestion: What is the key contribution of the article?\nAnswer: The key contributions are:\n\n- Introducing the first approach to context-aware machine translation using monolingual document-", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " The article does not provide any specific metrics for evaluation. The metrics used for evaluation are not mentioned. The article only provides results for two cross-lingual tasks (XNLI and UD parsing) and a supervised dependency parsing task.\n\n\nSo the answer is \"unanswerable\" - the question cannot be answered based on the information in the article.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is task-specific, three attentions are defined.\n\nThe attention module is task-specific, three attentions are defined. Specifically, the attention module is pre-trained on ASR, MT, and ST tasks.\n\n\nSo the answer is \"The attention module is pre-trained on ASR, MT, and ST tasks.\"", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " The kind of stylistic features obtained are:\n\n- Unigrams and Pragmatic features\n- BREF0 , BIBREF1 , BIBREF2 , BIBREF3 (b) Stylistic patterns BREF4 and related to situational disparity BREF5 and BIBREF6 (c) Hastag interpretations BREF6 , BREF7\n\n\n- Linguistic features like emoticons, laughter expressions, etc.\n\n- Distant supervision based techniques leveraging hashtags and stylistic/pragmatic features\n- St", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The encoder has an LSTM encoder that encodes the entire context and an LSTM decoder that generates the word form.\n\nQuestion: What is the baseline system?\nAnswer: The baseline system is an encoder-decoder on character sequences that takes a lemma as input and generates a word form. The context is conditioned on the lemma, word form and MSD of the previous and following word.\n\nQuestion: What is the auxiliary objective?\nAnswer: The auxiliary objective is meant to increase the morpho-syntactic awareness of the encoder and regular", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes, WordNet is useful for taxonomic reasoning for this task. The article states that WordNet is an English lexical database consisting of around 17k concepts, which are organized into groups of synsets that each contain a gloss (i.e., a definition), a set of representative words (called lemmas), and around 3k synsets, example sentences. In addition, many synsets have ISA links to other synsets that express complex taxonomic relations. The article goes on to explain how WordNet is used to construct challenge datasets for probing neural models. Specifically, it uses Word", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baselines mentioned in the article are:\n\n- End-to-end models: LibriSpeech and 2000 Fisher+Switchboard tasks\n- LibriSpeech test-other\n- LibriSpeech test-clean\n-other\n- Wall Street Journal (WSJ)\n- Hub5 Year 2000 (Hub5'00) evaluation (LDC002S09, LDC05S13)\n\nSo the baselines are end-to-end models on LibriSpeech and 2000 Fisher+", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": " The article does not provide any information about how many users look at the article. It does not mention the number of users who look at the article. Therefore, the question is unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The article mentions several metrics for evaluation, including:\n\n- BLEU-1/4: A metric for recipe generation quality that captures structural information via n-gram matching\n- ROUGE-L: A metric for recipe generation quality that captures subjective quality\n- BLEU-1: A metric for recipe generation that captures key entities (ingredient mentions)\n- BLEU-4: A metric for recipe generation that captures key entities (ingredient mentions)\n- BLEU-1/4: A metric for recipe generation that capt", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " The labels they create on their dataset are:\n\n- Open-ended Inquiry: Inquiries about well-being or particular symptom; e.g., “How are you feeling?” and “Do you cough?”\n\n- Detailed Inquiry: Inquiries with specific details that prompt yes/no answers or clarifications; e.g., “Do you cough at night?”\n- Multi-Intent Inquiry: Inquiring more than one symptom in a question; e.g., “Any cough, chest pain, or headache?”\n- Reconfirm", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The article does not provide any information about the amount of data needed to train the task-specific encoder. It only mentions that the encoder was fine-tuned during training. It does not specify how much data was used to train the encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " The article evaluates the adaptively sparse Transformer on four machine translation tasks:\n\n- IWSLT 2017 German-English\n-English\n- KFT Japanese-English\n- English\n- WMT 2016 Romanian-English\n-German\n- English\n\nThe article evaluates the adaptively sparse Transformer on these tasks using the following metrics:\n- BLEU scores\n- tokenized BLEU2 scores\n- for each task\n- tokenized BLEU2 scores\nThe article states that the adaptively sparse Transformer models tend to have slightly", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for Estonian in the NER task is not directly comparable to other languages, since the differences between the NER datasets depend more on the properties of the NER datasets than the quality of the embeddings. The authors state that the NER datasets are simplified to a common set of three labels (person, location, organization, and organization) and the number of words having each label is shown in Table 19. So the comparison between languages is not possible. The authors do not provide a direct comparison of ELMo embeddings for Estonian and other languages.\n\n\nThe authors state that", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " The article does not provide any information about the background of the authors. It does not mention their disciplinary backgrounds or affiliations.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " No, the paper is introducing a supervised approach to spam detection. The paper introduces a supervised approach to spam detection. The paper states:\n\n\"In this paper, we propose an efficient feature extraction method In this method, two topic-based features are extracted and used to discriminate human-like spammers from legitimate users.\"\n\nSo the question is \"Is LDA an unsupervised method?\" The answer is \"No\".", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni, Sotho, Sotho, Tshivenda, Xitsa and Xitsonga are similar to each other and harder to distinguish.\n\nThe article states:\n\n\"The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\"\n\n\nSo the answer is \"Nguni, Sotho, Tshivenda, Xitsa and Xitsonga\"", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " They compared 2-layers LSTM with 6-layers LSTM, 6-layers LSTM with Xavier initialization, 6-layers LSTM with 9-layers LSTM, 9-layers LSTM with Xavier initialization, and 9-layers LSTM with Xavier initialization.\n\n\nQuestion: What is the difference between layer-wise training and transfer learning?\nAnswer: In layer-wise training, the deeper model learns both parameters and knowledge from the shallower model. The deeper model is initialized by the shallower one and its alignment is the", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The data set consists of articles from English Wikipedia, with quality labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes in descending order of quality: Featured Article (FA), Good Article (GA), B-class Article (B), C-class Article (C), Start Article (Start), and Stub Article (Stub). A description of the criteria associated with the different classes can be found in the grading scheme page. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human judgments were assembled by a group of 50 native speakers of both English and Tamil who were well-versed in both languages. A collection of 10 sentences from the test set was taken for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. The intra-annotator values were computed for these metrics and the scores are shown in Table 2.\n\n\nQuestion: What is the learning rate decay rate used for the RNNMorph model?\nAnswer: The learning rate used for the RNNMorph model was ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " No, they do not test their framework on commonly used language pairs such as English-to-German. They test their framework on under-resourced translation and zero-resourced translation tasks.\n\n\nQuestion: What is the main difference between their approach and the work of BIBREF8?\n\nAnswer: The main difference between their approach and the work of BIBREF8 is that they need only one encoder for multiple target languages, while BIBREF8 needs one encoder per source language and one decoder per target language.\n\nQuestion: What is the advantage of using language-specific", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " The models are evaluated by training an autocomplete system on 50K randomly sampled sentences from Yelp reviews. The efficiency of a communication scheme is quantified by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The efficiency-accuracy tradeoff compared to two rule-based bas", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " The evaluation metrics looked at for classification tasks are precision, recall, F1-measure, and F-score.\n\n\nQuestion: What is the overall F-measure?\nAnswer: The overall F-measure is computed by averaging precision and recall.\n\nQuestion: What is the overall F-measure?\nAnswer: It is computed by averaging precision and recall.\n\nQuestion: What is the overall F-measure?\nAnswer: It is computed by averaging precision and recall.\n\nQuestion: What is the overall F-measure?\nAnswer: It is computed by averaging precision and recall.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the domain with labeled data, and the target domain is the unlabeled data.\n\nQuestion: What is the problem?\nAnswer: The problem is to bridge the source and target domains by learning domain-invariant representations so a classifier trained on the source can be adapted to another domain.\n\nQuestion: What are the limitations of previous methods?\nAnswer: They highly depend on heuristic pivot features, the learned representations may not effectively reduce the domain difference, and only utilize the unlabeled data for representation while the classifier was solely trained on the source.\n\nQuestion", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " They compare the PRU with state-of-the-art models including RNNs, LSTMs, RNNs, and LSTMs.\n\nQuestion: what is the pyramidal transformation?\nAnswer: The pyramidal transformation is a linear transformation that sub-samples the input vector INLINEFORM0 into INLINEFORM1 pyramidal levels.\n\nQuestion: what is the grouped linear transformation?\nAnswer: The grouped linear transformation is a linear transformation that splits the input vector INLINEFORM3 into smaller groups INLINEFORM4 .\n\nQuestion: what is the pyram", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " The article states that NeuronBlocks includes the following neural network modules:\n\n- Embedding Layer: Word/character embedding and extra handcrafted feature embedding such as pos-tagging\n- Neural Network Layers: Block zoo provides common layers like RNN, CNN, QRNN, Transformer, Encoder architecture, etc.\n- Attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full-Attention, Bidirectional attention, etc.\n- Loss Function: Besides the loss", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " They used the multilingual pronunciation corpus collected by deri016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table 10.\n\n\nQuestion: what metrics did they use to evaluate their model?\nAnswer: They used the following evaluation metrics:\n\n1. Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The baselines used in the paper are:\n\n- BERT (BIBREF12)\n- Uncased variant\n- bert-base-uncaseds3.amazonaws.com/models.huggingface.co/bert-uncased.tar.gz\n- RoBERTa (BIBREF2) - uncased variant- roberta-bases3.amazonaws.com/models.huggingface.co/roberta-pych_model.bin\n- XLNet (BREF1) - uncased variant- xlnet", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " They use English, Spanish, Finnish, and Spanish.\n\nQuestion: What is the name of the dataset they use for NLI?\nAnswer: MultiNLI\nQuestion: What is the name of the dataset they use for QA?\nAnswer: SQuAD\n\nQuestion: What is the name of the dataset they use for QA?\nAnswer: MLQA\nQuestion: What is the name of the dataset they use for NLI?\nAnswer: XNLI\nQuestion: What is the name of the dataset they use for QA?\nAnswer: MLQA\n\nQuestion:", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
