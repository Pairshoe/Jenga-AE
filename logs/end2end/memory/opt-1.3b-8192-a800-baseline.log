original_num_embeddings: 2048
original_embed_positions: torch.Size([2050, 2048])
[2025-05-05 23:31:36,985] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
before forward allocaiton: 2545.78564453125, peak memory: 2545.78564453125, reserve: 2554.0
after forward allocaiton: 22654.55810546875, peak memory: 23440.01123046875, reserve: 23542.0
before backward allocaiton: 18011.65185546875, peak memory: 22654.55810546875, reserve: 23542.0
after backward allocaiton: 2574.0361328125, peak memory: 19582.3681640625, reserve: 23542.0
before step allocaiton: 2574.03662109375, peak memory: 2574.0390625, reserve: 23542.0
after step allocaiton: 2598.03662109375, peak memory: 2610.03662109375, reserve: 23542.0
{'loss': 4.1001, 'grad_norm': 7.386586666107178, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
before forward allocaiton: 2586.03662109375, peak memory: 2598.03662109375, reserve: 23542.0
after forward allocaiton: 22686.68408203125, peak memory: 23472.13720703125, reserve: 23566.0
before backward allocaiton: 18043.77783203125, peak memory: 22686.68408203125, reserve: 23566.0
after backward allocaiton: 2598.037109375, peak memory: 19614.494140625, reserve: 23566.0
before step allocaiton: 2598.03662109375, peak memory: 2598.03955078125, reserve: 23566.0
after step allocaiton: 2598.03662109375, peak memory: 2610.03662109375, reserve: 23566.0
{'loss': 3.1826, 'grad_norm': 7.336878299713135, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
before forward allocaiton: 2586.03662109375, peak memory: 2598.03662109375, reserve: 23566.0
after forward allocaiton: 22686.68408203125, peak memory: 23472.13720703125, reserve: 23566.0
before backward allocaiton: 18043.77783203125, peak memory: 22686.68408203125, reserve: 23566.0
after backward allocaiton: 2598.037109375, peak memory: 19614.494140625, reserve: 23566.0
before step allocaiton: 2598.03662109375, peak memory: 2598.03955078125, reserve: 23566.0
after step allocaiton: 2598.03662109375, peak memory: 2610.03662109375, reserve: 23566.0
{'loss': 3.6447, 'grad_norm': 5.321752071380615, 'learning_rate': 3e-06, 'epoch': 0.0}
before forward allocaiton: 2586.03662109375, peak memory: 2598.03662109375, reserve: 23566.0
after forward allocaiton: 22686.68408203125, peak memory: 23472.13720703125, reserve: 23566.0
before backward allocaiton: 18043.77783203125, peak memory: 22686.68408203125, reserve: 23566.0
after backward allocaiton: 2598.037109375, peak memory: 19614.494140625, reserve: 23566.0
before step allocaiton: 2598.03662109375, peak memory: 2598.03955078125, reserve: 23566.0
after step allocaiton: 2598.03662109375, peak memory: 2610.03662109375, reserve: 23566.0
