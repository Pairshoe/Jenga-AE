original_num_embeddings: 2048
original_embed_positions: torch.Size([2050, 4096])
[2025-05-05 23:45:44,259] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
before forward allocaiton: 12748.10205078125, peak memory: 12748.10205078125, reserve: 12756.0
after forward allocaiton: 36267.40185546875, peak memory: 38708.12841796875, reserve: 38762.0
before backward allocaiton: 31385.94873046875, peak memory: 36267.40185546875, reserve: 38762.0
after backward allocaiton: 12796.3525390625, peak memory: 32171.2119140625, reserve: 38762.0
before step allocaiton: 12796.35302734375, peak memory: 12796.35546875, reserve: 38762.0
after step allocaiton: 12860.35302734375, peak memory: 12892.35302734375, reserve: 38848.0
{'loss': 7.317, 'grad_norm': 4.975824356079102, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
before forward allocaiton: 12828.35302734375, peak memory: 12860.35302734375, reserve: 38848.0
after forward allocaiton: 36339.52783203125, peak memory: 38780.25439453125, reserve: 38848.0
before backward allocaiton: 31458.07470703125, peak memory: 36339.52783203125, reserve: 38848.0
after backward allocaiton: 12860.353515625, peak memory: 32243.337890625, reserve: 38848.0
before step allocaiton: 12860.35302734375, peak memory: 12860.35595703125, reserve: 38848.0
after step allocaiton: 12860.35302734375, peak memory: 12892.35302734375, reserve: 38848.0
{'loss': 7.5958, 'grad_norm': 10.197757720947266, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
before forward allocaiton: 12828.35302734375, peak memory: 12860.35302734375, reserve: 38848.0
after forward allocaiton: 36339.52783203125, peak memory: 38780.25439453125, reserve: 38848.0
before backward allocaiton: 31458.07470703125, peak memory: 36339.52783203125, reserve: 38848.0
after backward allocaiton: 12860.353515625, peak memory: 32243.337890625, reserve: 38848.0
before step allocaiton: 12860.35302734375, peak memory: 12860.35595703125, reserve: 38848.0
after step allocaiton: 12860.35302734375, peak memory: 12892.35302734375, reserve: 38848.0
{'loss': 7.5429, 'grad_norm': 3.7545971870422363, 'learning_rate': 3e-06, 'epoch': 0.0}
before forward allocaiton: 12828.35302734375, peak memory: 12860.35302734375, reserve: 38848.0
after forward allocaiton: 36339.52783203125, peak memory: 38780.25439453125, reserve: 38848.0
before backward allocaiton: 31458.07470703125, peak memory: 36339.52783203125, reserve: 38848.0
after backward allocaiton: 12860.353515625, peak memory: 32243.337890625, reserve: 38848.0
before step allocaiton: 12860.35302734375, peak memory: 12860.35595703125, reserve: 38848.0
after step allocaiton: 12860.35302734375, peak memory: 12892.35302734375, reserve: 38848.0
