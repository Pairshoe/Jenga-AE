original_num_embeddings: 2048
original_embed_positions: torch.Size([2050, 4096])
[2025-05-05 23:48:07,008] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
before forward allocaiton: 12780.19580078125, peak memory: 12780.19580078125, reserve: 12796.0
after forward allocaiton: 59793.12841796875, peak memory: 64674.58154296875, reserve: 64748.0
before backward allocaiton: 50030.22216796875, peak memory: 59793.12841796875, reserve: 64748.0
after backward allocaiton: 12828.4462890625, peak memory: 51600.9384765625, reserve: 64748.0
before step allocaiton: 12828.44677734375, peak memory: 12828.44921875, reserve: 64748.0
after step allocaiton: 12892.44677734375, peak memory: 12924.44677734375, reserve: 64804.0
{'loss': 8.1417, 'grad_norm': 511.75830078125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
before forward allocaiton: 12860.44677734375, peak memory: 12892.44677734375, reserve: 64804.0
after forward allocaiton: 59865.25439453125, peak memory: 64746.70751953125, reserve: 64832.0
before backward allocaiton: 50102.34814453125, peak memory: 59865.25439453125, reserve: 64832.0
after backward allocaiton: 12892.447265625, peak memory: 51673.064453125, reserve: 64832.0
before step allocaiton: 12892.44677734375, peak memory: 12892.44970703125, reserve: 64832.0
after step allocaiton: 12892.44677734375, peak memory: 12924.44677734375, reserve: 64832.0
{'loss': 7.265, 'grad_norm': 460.65087890625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
before forward allocaiton: 12860.44677734375, peak memory: 12892.44677734375, reserve: 64832.0
after forward allocaiton: 59865.25439453125, peak memory: 64746.70751953125, reserve: 64832.0
before backward allocaiton: 50102.34814453125, peak memory: 59865.25439453125, reserve: 64832.0
after backward allocaiton: 12892.447265625, peak memory: 51673.064453125, reserve: 64832.0
before step allocaiton: 12892.44677734375, peak memory: 12892.44970703125, reserve: 64832.0
after step allocaiton: 12892.44677734375, peak memory: 12924.44677734375, reserve: 64832.0
{'loss': 7.0062, 'grad_norm': 270.0437316894531, 'learning_rate': 3e-06, 'epoch': 0.0}
before forward allocaiton: 12860.44677734375, peak memory: 12892.44677734375, reserve: 64832.0
after forward allocaiton: 59865.25439453125, peak memory: 64746.70751953125, reserve: 64832.0
before backward allocaiton: 50102.34814453125, peak memory: 59865.25439453125, reserve: 64832.0
after backward allocaiton: 12892.447265625, peak memory: 51673.064453125, reserve: 64832.0
before step allocaiton: 12892.44677734375, peak memory: 12892.44970703125, reserve: 64832.0
after step allocaiton: 12892.44677734375, peak memory: 12924.44677734375, reserve: 64832.0
